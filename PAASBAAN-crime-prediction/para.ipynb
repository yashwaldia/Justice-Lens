{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "data = pd.read_csv(\"C:/Users/Yash Waldia/Desktop/crime1/PAASBAAN-crime-prediction/preprocessed_data.csv\")\n",
    "\n",
    "# Convert Timestamp to datetime\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Extract date-time features\n",
    "db = pd.DataFrame({\n",
    "    \"year\": data['Timestamp'].dt.year,\n",
    "    \"month\": data['Timestamp'].dt.month,\n",
    "    \"day\": data['Timestamp'].dt.day,\n",
    "    \"hour\": data['Timestamp'].dt.hour,\n",
    "    \"week\": data['Timestamp'].dt.isocalendar().week,  # Use isocalendar().week\n",
    "    \"weekday\": data['Timestamp'].dt.weekday,\n",
    "    \"dayofyear\": data['Timestamp'].dt.dayofyear,\n",
    "    \"quarter\": data['Timestamp'].dt.quarter,\n",
    "})\n",
    "\n",
    "# Concatenate the extracted features with the original dataset\n",
    "data1 = pd.concat([db, data.drop('Timestamp', axis=1)], axis=1)\n",
    "\n",
    "# Split data into features (X) and target variables (y)\n",
    "# X = data1[['year', 'month', 'day', 'hour','week', 'weekday', 'dayofyear', 'Lat', 'Long']]\n",
    "X = data1[['year', 'month', 'day', 'hour', 'Lat', 'Long']]\n",
    "y = data1[['Accident', 'Drug Violation', 'Harassment', 'Robbery']]\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameters grid for tuning\n",
    "param_grid = {\n",
    "  'n_estimators': [50, 100],\n",
    "  'max_depth': [10, 15, 20],  # Reduced options for max_depth\n",
    "  'min_samples_split': [2, 5],\n",
    "  'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Perform Grid Search CV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Initialize Random Forest Classifier with the best hyperparameters\n",
    "rfc_best = RandomForestClassifier(**best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "rfc_best.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = rfc_best.score(X_train, y_train)\n",
    "test_accuracy = rfc_best.score(X_test, y_test)\n",
    "\n",
    "print(\"Training accuracy:\", train_accuracy)\n",
    "print(\"Testing accuracy:\", test_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(rfc_best, 'rf_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "data = pd.read_csv(\"C:/Users/Yash Waldia/Desktop/crime1/PAASBAAN-crime-prediction/pd.csv\")\n",
    "\n",
    "# Split data into features (X) and target variables (y)\n",
    "X = data[['YEAR', 'MONTH', 'DAY', 'HOUR', 'Latitude', 'Longitude']]\n",
    "y = data[['crime1', 'crime2', 'crime3', 'crime4']]\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameters grid for tuning\n",
    "param_grid = {\n",
    "  'n_estimators': [50, 100],\n",
    "  'max_depth': [10, 15, 20],  # Reduced options for max_depth\n",
    "  'min_samples_split': [2, 5],\n",
    "  'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Perform Grid Search CV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Initialize Random Forest Classifier with the best hyperparameters\n",
    "rfc_best = RandomForestClassifier(**best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "rfc_best.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = rfc_best.score(X_train, y_train)\n",
    "test_accuracy = rfc_best.score(X_test, y_test)\n",
    "\n",
    "print(\"Training accuracy:\", train_accuracy)\n",
    "print(\"Testing accuracy:\", test_accuracy)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(rfc_best, 'rf_model3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Read the preprocessed dataset\n",
    "data = pd.read_csv(\"C:/Users/Yash Waldia/Desktop/crime1/PAASBAAN-crime-prediction/pd.csv\")\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(['crime1', 'crime2', 'crime3', 'crime4'], axis=1)  # Features\n",
    "y = data[['crime1', 'crime2', 'crime3', 'crime4']]  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors as needed\n",
    "\n",
    "# Train the KNN classifier\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the labels for test set\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:/Users/Yash Waldia/Desktop/crime1/PAASBAAN-crime-prediction/pd.csv\")\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data[['YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE','Latitude', 'Longitude']]\n",
    "y = data[['crime1', 'crime2', 'crime3', 'crime4']]  # Assuming you have four types of crimes\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Now, you can use this trained model to predict crimes for new data\n",
    "# For example:\n",
    "new_data = pd.DataFrame({\n",
    "    'YEAR': [2024],\n",
    "    'MONTH': [5],\n",
    "    'DAY': [30],\n",
    "    'HOUR': [14],\n",
    "    'MINUTE': [30],\n",
    "    'Latitude': [491277.3697],\n",
    "    'Longitude': [5458444.38]\n",
    "})\n",
    "\n",
    "predicted_crimes = rfc.predict(new_data)\n",
    "print(\"Predicted crimes for the new data:\", predicted_crimes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"pd2.csv\")\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data[['YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'Latitude', 'Longitude','NEIGHBOURHOOD_ID']]\n",
    "y = data[['crime1', 'crime2', 'crime3', 'crime4']]  # Assuming you have four types of crimes\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Now, you can use this trained model to predict crimes for new data\n",
    "# For example:\n",
    "new_data = pd.DataFrame({\n",
    "    'YEAR': [2025],\n",
    "    'MONTH': [5],\n",
    "    'DAY': [3],\n",
    "    'HOUR': [14],\n",
    "    'MINUTE': [30],\n",
    "    'Latitude': [49.26980201],\n",
    "    'Longitude': [-123.0837633],\n",
    "    'NEIGHBOURHOOD_ID': [8]\n",
    "})\n",
    "\n",
    "predicted_crimes = knn.predict(new_data)\n",
    "print(\"Predicted crimes for the new data:\", predicted_crimes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"pd2.csv\")\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data[['YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'Latitude', 'Longitude','NEIGHBOURHOOD_ID']]\n",
    "y = data[['crime1', 'crime2', 'crime3', 'crime4']]  # Assuming you have four types of crimes\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Now, you can use this trained model to predict crimes for new data\n",
    "# For example:\n",
    "new_data = pd.DataFrame({\n",
    "    'YEAR': [2026],\n",
    "    'MONTH': [4],\n",
    "    'DAY': [30],\n",
    "    'HOUR': [14],\n",
    "    'MINUTE': [30],\n",
    "    'Latitude': [49.26980201],\n",
    "    'Longitude': [-123.0837633],\n",
    "    'NEIGHBOURHOOD_ID': [16]\n",
    "})\n",
    "\n",
    "predicted_crimes = dt.predict(new_data)\n",
    "print(\"Predicted crimes for the new data:\", predicted_crimes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset into a DataFrame\n",
    "# Assuming your dataset is stored in a CSV file named 'pd.csv'\n",
    "df = pd.read_csv('pd.csv')\n",
    "\n",
    "# Calculate the total number of entries for each crime type\n",
    "crime_counts = df.iloc[:, 7:].sum()\n",
    "\n",
    "# Determine the smallest count among the crime types\n",
    "min_count = crime_counts.min()\n",
    "\n",
    "# Randomly sample entries from each crime type to match the count of the smallest crime type\n",
    "sampled_data = pd.concat([df[df[f'crime{i+1}'] == 1].sample(min_count, replace=True) for i in range(4)])\n",
    "\n",
    "# Shuffle the sampled data\n",
    "sampled_data = sampled_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Reduce the dataset to about 10 thousand entries\n",
    "final_data = sampled_data.sample(n=min(10000, len(sampled_data)))\n",
    "\n",
    "# Save or use the 'final_data' DataFrame as your reduced dataset\n",
    "final_data.to_csv('reduced_crime_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yash Waldia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.3526 - loss: 1.3390 - val_accuracy: 0.4145 - val_loss: 1.2714\n",
      "Epoch 2/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4194 - loss: 1.2684 - val_accuracy: 0.4225 - val_loss: 1.2563\n",
      "Epoch 3/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4168 - loss: 1.2614 - val_accuracy: 0.4355 - val_loss: 1.2394\n",
      "Epoch 4/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4378 - loss: 1.2326 - val_accuracy: 0.4355 - val_loss: 1.2261\n",
      "Epoch 5/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4320 - loss: 1.2238 - val_accuracy: 0.4390 - val_loss: 1.2185\n",
      "Epoch 6/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4380 - loss: 1.2208 - val_accuracy: 0.4410 - val_loss: 1.2145\n",
      "Epoch 7/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4459 - loss: 1.2165 - val_accuracy: 0.4470 - val_loss: 1.2125\n",
      "Epoch 8/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4503 - loss: 1.2142 - val_accuracy: 0.4525 - val_loss: 1.2106\n",
      "Epoch 9/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4469 - loss: 1.2083 - val_accuracy: 0.4510 - val_loss: 1.2103\n",
      "Epoch 10/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4573 - loss: 1.2018 - val_accuracy: 0.4430 - val_loss: 1.2090\n",
      "Epoch 11/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4541 - loss: 1.2100 - val_accuracy: 0.4490 - val_loss: 1.2073\n",
      "Epoch 12/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4560 - loss: 1.1978 - val_accuracy: 0.4560 - val_loss: 1.2053\n",
      "Epoch 13/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4539 - loss: 1.1943 - val_accuracy: 0.4580 - val_loss: 1.2056\n",
      "Epoch 14/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4628 - loss: 1.1916 - val_accuracy: 0.4570 - val_loss: 1.2072\n",
      "Epoch 15/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4579 - loss: 1.1983 - val_accuracy: 0.4585 - val_loss: 1.2033\n",
      "Epoch 16/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4574 - loss: 1.1990 - val_accuracy: 0.4545 - val_loss: 1.2027\n",
      "Epoch 17/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4612 - loss: 1.1926 - val_accuracy: 0.4575 - val_loss: 1.2017\n",
      "Epoch 18/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4715 - loss: 1.1907 - val_accuracy: 0.4600 - val_loss: 1.2018\n",
      "Epoch 19/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4646 - loss: 1.1921 - val_accuracy: 0.4595 - val_loss: 1.2003\n",
      "Epoch 20/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4616 - loss: 1.1904 - val_accuracy: 0.4655 - val_loss: 1.1989\n",
      "Epoch 21/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4687 - loss: 1.1925 - val_accuracy: 0.4635 - val_loss: 1.1972\n",
      "Epoch 22/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4677 - loss: 1.1850 - val_accuracy: 0.4660 - val_loss: 1.1971\n",
      "Epoch 23/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4605 - loss: 1.1828 - val_accuracy: 0.4610 - val_loss: 1.1954\n",
      "Epoch 24/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4717 - loss: 1.1783 - val_accuracy: 0.4665 - val_loss: 1.1953\n",
      "Epoch 25/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4721 - loss: 1.1772 - val_accuracy: 0.4670 - val_loss: 1.1929\n",
      "Epoch 26/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4729 - loss: 1.1748 - val_accuracy: 0.4700 - val_loss: 1.1915\n",
      "Epoch 27/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4738 - loss: 1.1806 - val_accuracy: 0.4685 - val_loss: 1.1885\n",
      "Epoch 28/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4776 - loss: 1.1761 - val_accuracy: 0.4690 - val_loss: 1.1875\n",
      "Epoch 29/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4780 - loss: 1.1647 - val_accuracy: 0.4755 - val_loss: 1.1854\n",
      "Epoch 30/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4790 - loss: 1.1700 - val_accuracy: 0.4730 - val_loss: 1.1834\n",
      "Epoch 31/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4838 - loss: 1.1676 - val_accuracy: 0.4675 - val_loss: 1.1840\n",
      "Epoch 32/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4776 - loss: 1.1606 - val_accuracy: 0.4745 - val_loss: 1.1813\n",
      "Epoch 33/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4826 - loss: 1.1674 - val_accuracy: 0.4655 - val_loss: 1.1792\n",
      "Epoch 34/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4945 - loss: 1.1509 - val_accuracy: 0.4725 - val_loss: 1.1800\n",
      "Epoch 35/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4858 - loss: 1.1550 - val_accuracy: 0.4760 - val_loss: 1.1767\n",
      "Epoch 36/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4918 - loss: 1.1514 - val_accuracy: 0.4715 - val_loss: 1.1769\n",
      "Epoch 37/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4933 - loss: 1.1542 - val_accuracy: 0.4740 - val_loss: 1.1730\n",
      "Epoch 38/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4907 - loss: 1.1528 - val_accuracy: 0.4780 - val_loss: 1.1706\n",
      "Epoch 39/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4914 - loss: 1.1482 - val_accuracy: 0.4795 - val_loss: 1.1700\n",
      "Epoch 40/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4918 - loss: 1.1484 - val_accuracy: 0.4800 - val_loss: 1.1696\n",
      "Epoch 41/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4926 - loss: 1.1447 - val_accuracy: 0.4815 - val_loss: 1.1671\n",
      "Epoch 42/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4861 - loss: 1.1498 - val_accuracy: 0.4810 - val_loss: 1.1666\n",
      "Epoch 43/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4891 - loss: 1.1471 - val_accuracy: 0.4800 - val_loss: 1.1664\n",
      "Epoch 44/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4948 - loss: 1.1338 - val_accuracy: 0.4810 - val_loss: 1.1629\n",
      "Epoch 45/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4904 - loss: 1.1395 - val_accuracy: 0.4840 - val_loss: 1.1627\n",
      "Epoch 46/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4945 - loss: 1.1407 - val_accuracy: 0.4870 - val_loss: 1.1601\n",
      "Epoch 47/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4876 - loss: 1.1449 - val_accuracy: 0.4840 - val_loss: 1.1605\n",
      "Epoch 48/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4859 - loss: 1.1392 - val_accuracy: 0.4840 - val_loss: 1.1611\n",
      "Epoch 49/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4963 - loss: 1.1420 - val_accuracy: 0.4860 - val_loss: 1.1603\n",
      "Epoch 50/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4946 - loss: 1.1323 - val_accuracy: 0.4825 - val_loss: 1.1601\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4747 - loss: 1.1806\n",
      "Accuracy: 48.25%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"reduced_crime_data.csv\")\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data[['YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'Latitude', 'Longitude']]\n",
    "y = data[['crime1', 'crime2', 'crime3', 'crime4']]  # Assuming you have four types of crimes\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the data for LSTM input [samples, time steps, features]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(4, activation='softmax'))  # Assuming four types of crimes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f%%' % (accuracy * 100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yash Waldia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step\n",
      "Predicted probabilities of crimes for the new data: [[0.2177617  0.18224554 0.5397655  0.06022722]]\n"
     ]
    }
   ],
   "source": [
    "# Now, you can use this trained model to predict crimes for new data\n",
    "# For example:\n",
    "new_data = np.array([[2026, 8, 13, 4, 20, 49.16980201, -123.0837633]])\n",
    "new_data = scaler.transform(new_data)\n",
    "new_data = np.reshape(new_data, (1, 1, new_data.shape[1]))\n",
    "\n",
    "predicted_crimes = model.predict(new_data)\n",
    "print(\"Predicted probabilities of crimes for the new data:\", predicted_crimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file in the native Keras format\n",
    "model.save(\"crime_prediction_lstm_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"reduced_crime_data.csv\")\n",
    "\n",
    "# Sum up the occurrences of each crime type\n",
    "total_crime1 = data['crime1'].sum()\n",
    "total_crime2 = data['crime2'].sum()\n",
    "total_crime3 = data['crime3'].sum()\n",
    "total_crime4 = data['crime4'].sum()\n",
    "\n",
    "# Print the total occurrences of each crime type\n",
    "print(\"Total occurrences of Crime 1:\", total_crime1)\n",
    "print(\"Total occurrences of Crime 2:\", total_crime2)\n",
    "print(\"Total occurrences of Crime 3:\", total_crime3)\n",
    "print(\"Total occurrences of Crime 4:\", total_crime4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, you can use this trained model to predict crimes for new data\n",
    "# For example:\n",
    "new_data = np.array([[2026, 8, 13, 4, 20, 49.16980201, -123.0837633]])\n",
    "new_data = scaler.transform(new_data)\n",
    "new_data = np.reshape(new_data, (1, 1, new_data.shape[1]))\n",
    "\n",
    "predicted_crimes = model.predict(new_data)\n",
    "print(\"Predicted probabilities of crimes for the new data:\", predicted_crimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
